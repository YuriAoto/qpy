#!/usr/bin/python
# qpy-master - set the main driver for qpy
#
# 29 May 2015 - Pradipta and Yuri
from multiprocessing.connection import Listener
from multiprocessing.connection import Client
import threading
from time import sleep
from Queue import Queue
import re
import subprocess
import sys
import os
import random
from optparse import OptionParser

parser = OptionParser()
parser.add_option("-v", "--verbose",
                  action="store_true", dest="verbose", default=False,
                  help="print messages")

parser.add_option("-c", "--cluster",
                  dest="cl",
                  help="the cluster (linux4 and hlrs available)")


(options, args) = parser.parse_args()

verbose = options.verbose
cluster = options.cl

if (not (cluster)):
    sys.exit( 'give the --cluster option')

if (cluster != "hlrs" and cluster != "linux4"):
    sys.exit( 'cluster must be hlrs or linux4')

dyn_nodes = cluster == "hlrs"

max_nodes_default = -1
if (dyn_nodes):
    max_nodes_default = 3


home_dir = os.environ['HOME']
qpy_dir = os.path.dirname(os.path.abspath(__file__))

random.seed()
#port = 16000# random.randint( 10000, 20000 )
port = random.randint( 10000, 20000 )
f_port = open( qpy_dir + '/.port', 'w', 0)
f_port.write( str( port))
f_port.close()

def node_alloc():
    queue_script = 'qpy --alloc'
    command = 'salloc -N1 -t 21-0 -K -A ithkoehn ' + queue_script + ' &'
    salloc = subprocess.Popen(command,
                              shell=True,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
    salloc_stderr = salloc.stderr.readline()
    re_res = re.match('salloc: Granted job allocation (\d+)', salloc_stderr)
    try:
        job_id = re_res.group(1)
    except:
        if (verbose):
            print 'Error in node allocation: ' + salloc_stderr
        return None
    command = 'squeue | grep ' + job_id
    squeue = subprocess.Popen(command,
                              shell=True,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
    saloc_stdout = squeue.stdout.readline().split()
    node = saloc_stdout[-1]

    while (True):

        init_script = 'source ~/.bash_profile; qpy --init'
        alloc = subprocess.Popen(["ssh", "-o", "StrictHostKeyChecking=no", node, init_script],
                                 shell=False,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)

        alloc_stdout = alloc.stdout.readlines()
        alloc_stderr = alloc.stderr.readlines()
        if (verbose):
            print "alloc stdout:", alloc_stdout
            print "alloc stderr:", alloc_stderr

        error = False
        for i in alloc_stderr:
            if ('No route to host' in i or 'Connection refused' in i):
                error = True

        success = False
        for i in alloc_stdout:
            if ('Success on qpy init!' in i):
                success = True

        if (not (error) and success):
            break

        if (verbose):
            print "Waiting node initialization: " + node

        sleep( 10)

    
    if (verbose):
        print 'Node ' + node + ' allocated.'

    return (job_id, node)


def node_dealloc( node_id, alloc_id):
    term_script = 'source ~/.bash_profile; qpy --term'
    dealloc = subprocess.Popen(["ssh", "-o", "StrictHostKeyChecking=no", node_id, term_script],
                               shell=False,
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)

    if (verbose):
        print "dealloc stdout:", dealloc.stdout.readlines()
        print "dealloc stderr:", dealloc.stderr.readlines()

    command = 'scancel ' + alloc_id
    dealloc = subprocess.call( command, shell=True)

    if (verbose):
        print 'Node ' + node_id + ' deallocated.'

# Check output
#
class out_err_check( threading.Thread):
    
    def __init__( self, ssh, out_or_err, out_err_file):
        threading.Thread.__init__( self)
        self.ssh = ssh
        self.out_or_err = out_or_err
        self.file = out_err_file
        self.file_lock = threading.RLock()

    def run( self):

        if (self.out_or_err):
            for c in iter( lambda: self.ssh.stdout.read(1), ''):
                self.file_lock.acquire()
                self.file.write(c)
                self.file_lock.release()
        else:
            for c in iter( lambda: self.ssh.stderr.read(1), ''):
                self.file_lock.acquire()
                self.file.write(c)
                self.file_lock.release()

    
# Job sender - send a job to a node
#
class job_send( threading.Thread):
    
    def __init__( self, node, job, jobID):
        threading.Thread.__init__( self)
        self.node = node
        self.job = job
        self.status = 'running'
        self.jobID = jobID
        self.err_msg = ''

    def run( self):
        command = 'echo qpy-jobID ' + str( self.jobID) + '; source ~/.bash_profile; cd ' + self.job[1] + '; ' + self.job[0]
        ssh = subprocess.Popen(["ssh", self.node, command],
                               shell=False,
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)

        f_out_err = open( self.job[1] + '/job_' + str(self.jobID) + '.outerr', 'w', 0)

        err_chk = out_err_check( ssh, False, f_out_err)
        err_chk.start()
        out_chk = out_err_check( ssh, True, f_out_err)
        out_chk.start()

        err_chk.join()
        out_chk.join()

        f_out_err.close()


# A node.
#
class node_connection( threading.Thread):

    def __init__( self, max_jobs, *args):
        threading.Thread.__init__( self)

        if (dyn_nodes):
            new_node = node_alloc()
        else:
            if ( not(args)):
                sys.exit( 'Internal error: no arguments in node allocation')
            new_node = ( None, args[0])
        try:
            self.alloc_id = new_node[0]
            self.node_id = new_node[1]
        except:
            self.alloc_id = None
            self.node_id = None

        self.max_jobs = max_jobs
        self.queue = Queue()
        self.jobs = []
        self.jobs_lock = threading.RLock()

        
    def run( self):

        while True:
            new_job = self.queue.get()
            if (verbose):
                print 'Node ' + str( self.node_id) + ': new job.'

            if ( isinstance( new_job, str)):
                if ( new_job == 'kill'):
                    break

            # new_job = [<jobID>, <job_type>, <job_info>]
            j = job_send( self.node_id, new_job[2], new_job[0])
            j.start()
            self.jobs_lock.acquire()
            self.jobs.append(j)
            self.jobs_lock.release()
                
            sleep(1)

        if ( self.alloc_id != None):
            node_dealloc( self.node_id, self.alloc_id)
        
        if (verbose):
            print 'Node ' + str( self.node_id) + ' done.'
            

# Control jobs subimission
# 
#
class submission_control( threading.Thread):
    def __init__( self):
        threading.Thread.__init__( self)
        self.live = True
        self.node_list = []
        self.max_nodes = max_nodes_default
        self.max_jobs_default = 20
        self.jobs_queue = Queue()
        self.jobs_queue_lock = threading.RLock()
        self.jobs_done = []
        self.jobs_done_lock = threading.RLock()

    def run( self):
        skip_job_sub = 0
        while self.live:
            
            # Check finished jobs
            for node_c in self.node_list:
                node_c.jobs_lock.acquire()
                for running_job in node_c.jobs:
                    if (not (running_job.is_alive())):
                        if (running_job.status == 'running'):
                            running_job.status = 'normal'
                        self.jobs_done_lock.acquire()
                        self.jobs_done.append( running_job)
                        self.jobs_done_lock.release()
                        node_c.jobs.remove( running_job)
                        skip_job_sub = 0
                node_c.jobs_lock.release()

            # Dealloc the non-used node
            if (dyn_nodes):
                for node_c in self.node_list:
                    node_c.jobs_lock.acquire()
                    if (not( node_c.jobs)):
                        node_c.queue.put( 'kill')
                        self.node_list.remove( node_c)
                    node_c.jobs_lock.release()

            # Send a job, if there is space
            if (skip_job_sub <= 0):
                self.jobs_queue_lock.acquire()
                put_a_job = not( self.jobs_queue.empty())
                self.jobs_queue_lock.release()
                if (put_a_job):
                    best_free = 0
                    best_node = None
                    for node_c in self.node_list:
                        node_c.jobs_lock.acquire()
                        free = node_c.max_jobs - len(node_c.jobs)
                        node_c.jobs_lock.release()
                        if (free > best_free):
                            best_node = node_c
                            best_free = free
                    if (best_node == None and len( self.node_list) < self.max_nodes):
                        best_node = node_connection( self.max_jobs_default)
                        if (best_node.node_id):
                            best_node.start()
                            sub_ctrl.node_list.append( best_node)
                        else:
                            best_node.start()
                            best_node.queue.put( 'kill')
                            best_node = None
                            skip_job_sub = 100
                    if (best_node != None):
                        if (verbose):
                            print "submission control: putting job"
                        self.jobs_queue_lock.acquire()
                        best_node.queue.put( self.jobs_queue.get())
                        self.jobs_queue_lock.release()
            else:
                skip_job_sub = skip_job_sub - 1
                if (verbose):
                    print "Skipping job submission: " + str( skip_job_sub)

            sleep( 1)

# To sort by jobID in format_jobs
def get_jobID_to_sort( l):
    return l[0]

# formating a list of jobs
def format_jobs( jobs_list):

    list_tmp = []
    for j in jobs_list:
        if (j.status == 'running'):
            list_tmp.append( (j.jobID, str( j.jobID) + ':' + j.job[0] + ' (wd: ' + j.job[1] + ')\n'))
        else:
            list_tmp.append( (j.jobID, str( j.jobID) + ' (' + j.status + '):' + j.job[0] + ' (wd: ' + j.job[1] + ')\n'))

    list_tmp.sort( key = get_jobID_to_sort)

    jobs = ''
    for i in list_tmp:
        jobs += i[1];

    return jobs

# return string with current jobs
def get_cur_jobs( sub_ctrl):
    nd_jobs = ''
    cur_jobs = ''

    sub_ctrl.jobs_queue_lock.acquire()
    n_q = sub_ctrl.jobs_queue.qsize()
    for i in range( 0, n_q):
        j = sub_ctrl.jobs_queue.get()
        nd_jobs += str( j[0]) + ':' + j[2][0] + ' (wd: ' + j[2][1] + ')\n';
        sub_ctrl.jobs_queue.put( j)
    sub_ctrl.jobs_queue_lock.release()

    if (nd_jobs):
        cur_jobs += 'Queue' + ':\n' + nd_jobs

    for node in sub_ctrl.node_list:
        nd_jobs = format_jobs( node.jobs)
        if (nd_jobs):
            cur_jobs += node.node_id + ':\n' + nd_jobs
    return cur_jobs



# Kill job <ID> on node <node>
def kill_a_job( j):
    kill_command = 'source ~/.bash_profile; qpy --jobkill ' + str( j.jobID)
    kill_p = subprocess.Popen(["ssh", "-o", "StrictHostKeyChecking=no", j.node, kill_command],
                              shell=False,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
    j.status = 'killed'
    if (verbose):
        print "Killing job " + str( j.jobID) + ' on node ' + j.node
        print "kill_stdout: ", kill_p.stdout.readlines()
        print "kill_stderr: ", kill_p.stderr.readlines()



# Get job from client and send it for submission
# message from client must be:
#   (job_type, job)
# where:
#   job_type is
#       1 - submit a job     (sub)
#       2 - check the jobs   (check)
#       3 - kill a job       (kill)
#       4 - kill the master  (finish)
#       5 - change max_nodes (nodes)
#       6 - change max_jobs  (njobs)
#       7 - show config      (config)
#       8 - show jobs done   (done)
#              
#   job is a list with the job informations
#
def handle_client( sub_ctrl, jobId):
    if (verbose):
        print "queue_master_driver: ready"
    server_master = Listener(( "localhost", port), authkey = 'qwerty')
    while True:
        client_master = server_master.accept()
        (job_type, arguments) = client_master.recv()

        # Send a job
        if (job_type == 1):
            sub_ctrl.jobs_queue_lock.acquire()
            sub_ctrl.jobs_queue.put( (jobId, job_type, arguments))
            sub_ctrl.jobs_queue_lock.release()
            jobId += 1
            client_master.send( 'Job received.\n')

        # Check jobs
        elif (job_type == 2):
            client_master.send( get_cur_jobs( sub_ctrl))

        # Kill a job
        elif (job_type == 3):

            killall = False
            if ('all' == arguments):
                killall = True

            n_kill_queue = 0
            sub_ctrl.jobs_queue_lock.acquire()
            sub_ctrl.jobs_done_lock.acquire()
            n_q = sub_ctrl.jobs_queue.qsize()
            for i in range( 0, n_q):
                j = sub_ctrl.jobs_queue.get()
                if (not( killall) and not( j[0] in arguments)):
                    sub_ctrl.jobs_queue.put( j)
                else:
                    n_kill_queue += 1
            sub_ctrl.jobs_queue_lock.release()
            sub_ctrl.jobs_done_lock.release()

            n_kill_run = 0
            if (killall):
                for n in sub_ctrl.node_list:
                    for j in n.jobs:
                        kill_a_job( j)
                        n_kill_run += 1
            else:
                for i in arguments:
                    job_to_kill = ''
                    for n in sub_ctrl.node_list:
                        for j in n.jobs:
                            if (j.jobID == i):
                                job_to_kill = j
                                break
                    if (job_to_kill):
                        kill_a_job( job_to_kill)
                        n_kill_run += 1

            client_master.send( str(n_kill_queue) + ' jobs removed from the queue and ' + str(n_kill_run) + ' jobs killed.\n')

        # Finish the master execution
        if (job_type == 4):
            if (sub_ctrl.node_list or not( sub_ctrl.jobs_queue.empty())):
                client_master.send( 'There are unfinished jobs.\n')
            else:
                client_master.send( 'Stopping master driver.\n')
                sub_ctrl.live = False
                break

        # Change maximum number of nodes
        elif (job_type == 5):
            if (dyn_nodes):
                sub_ctrl.max_nodes = arguments
                client_master.send( 'Maximum number of nodes changed to ' + str( arguments) + '.\n')
            else:
                if (arguments):
                    new_node = node_connection( sub_ctrl.max_jobs_default, arguments)
                    new_node.start()
                    sub_ctrl.node_list.append( new_node)
                    client_master.send( 'Node ' + arguments + ' added.\n')
                else:
                    cur_nodes = ''
                    for node in sub_ctrl.node_list:
                        cur_nodes += node.node_id + ' ' + str( len( node.jobs)) + '/' + str( node.max_jobs) + '\n'
                    client_master.send( cur_nodes)


        # Change maximum number of jobs
        elif (job_type == 6):
            sub_ctrl.max_jobs_default = arguments
            for n in sub_ctrl.node_list:
                n.max_jobs = sub_ctrl.max_jobs_default
            client_master.send( 'Maximum number of jobs changed to ' + str( arguments) + '.\n')

        # Show current configuration
        elif (job_type == 7):
            cur_conf = ''
            cur_conf += 'max_jobs  = ' + str( sub_ctrl.max_jobs_default) + '\n'
            cur_conf += 'max_nodes = ' + str( sub_ctrl.max_nodes) + '\n'
            client_master.send( cur_conf)

        # make a list of finished jobs or remove them from the list
        elif (job_type == 8):
            if (arguments):
                n_jobs = 0
                sub_ctrl.jobs_done_lock.acquire()
                if (arguments == 'all'):
                    n_jobs = len( sub_ctrl.jobs_done)
                    sub_ctrl.jobs_done = []
                else:
                    for i in arguments:
                        for j in sub_ctrl.jobs_done:
                            if (i == j.jobID):
                                sub_ctrl.jobs_done.remove(j)
                                n_jobs += 1
                                break 
                    sub_ctrl.jobs_done_lock.release()
                client_master.send( str( n_jobs) + ' finished jobs are removed \n')
            else:
                cur_jobs = ''
                sub_ctrl.jobs_done_lock.acquire()
                jobs = format_jobs(sub_ctrl.jobs_done)
                sub_ctrl.jobs_done_lock.release()
                if (jobs):
                    cur_jobs += 'finished jobs' + ':\n' + jobs
                client_master.send(cur_jobs)

        else:
            client_master.send( 'Unknown option: ' + str( job_type) + '\n')


sub_ctrl = submission_control()
sub_ctrl.start()
handle_client( sub_ctrl, 1)
if (verbose):
    print "queue_master_driver: done!"

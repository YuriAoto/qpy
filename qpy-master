#!/usr/bin/python
# queue_master_driver.py
from multiprocessing.connection import Listener
from multiprocessing.connection import Client
import threading
from time import sleep
from Queue import Queue
import re
import subprocess
import sys
import os
import random

home_dir = os.environ['HOME']
queue_dir = home_dir + '/Codes/queue_hlrs'

random.seed()
port = random.randint( 10000, 20000 )
f_port = open(queue_dir + '/.port', 'w', 0)
f_port.write( str( port))
f_port.close()

def node_alloc():
    queue_script = 'queue'
    command = 'salloc -N1 -t 21-0 -K -A ithkoehn ' + queue_script + ' &'
    salloc = subprocess.Popen(command,
                              shell=True,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
    salloc_stderr = salloc.stderr.readline()
    re_res = re.match('salloc: Granted job allocation (\d+)', salloc_stderr)
    job_id = re_res.group(1)
    command = 'squeue | grep ' + job_id
    squeue = subprocess.Popen(command,
                              shell=True,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
    saloc_stdout = squeue.stdout.readline().split()
    node = saloc_stdout[-1]

    init_script = 'source ~/.bash_profile; init_q'
    alloc = subprocess.Popen(["ssh", "-o", "StrictHostKeyChecking=no", node, init_script],
                             shell=False,
                             stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
    print "stdout: ", alloc.stdout.readlines()
    print "stderr: ", alloc.stderr.readlines()
    
    return (job_id, node)


def node_dealloc( node_id, alloc_id):
    term_script = 'source ~/.bash_profile; term_q'
    dealloc = subprocess.Popen(["ssh", "-o", "StrictHostKeyChecking=no", node_id, term_script],
                               shell=False,
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)
    print "dealloc stdout: ", dealloc.stdout.readlines()
    print "dealloc stderr: ", dealloc.stderr.readlines()

    command = 'scancel ' + alloc_id
    dealloc = subprocess.call( command, shell=True)

# Check output
#
class out_err_check( threading.Thread):
    
    def __init__( self, ssh, out_or_err, out_err_file):
        threading.Thread.__init__( self)
        self.ssh = ssh
        self.out_or_err = out_or_err
        self.file = out_err_file
        self.file_lock = threading.RLock()

    def run( self):

        if (self.out_or_err):
            for c in iter( lambda: self.ssh.stdout.read(1), ''):
                self.file_lock.acquire()
                self.file.write(c)
                self.file_lock.release()
        else:
            for c in iter( lambda: self.ssh.stderr.read(1), ''):
                self.file_lock.acquire()
                self.file.write(c)
                self.file_lock.release()

    
# Job sender - send a job to a node
#
class job_send( threading.Thread):
    
    def __init__( self, node, job, jobID):
        threading.Thread.__init__( self)
        self.node = node
        self.job = job
        self.jobID = jobID
        self.err_msg = ''

    def run( self):
        command = 'source ~/.bash_profile; cd ' + self.job[1] + '; ' + self.job[0]
        ssh = subprocess.Popen(["ssh", self.node, command],
                               shell=False,
                               stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE)
        print ssh.pid
        f_out_err = open( self.job[1] + '/job_' + str(self.jobID) + '.outerr', 'w', 0)

        err_chk = out_err_check( ssh, False, f_out_err)
        err_chk.start()
        out_chk = out_err_check( ssh, True, f_out_err)
        out_chk.start()

        err_chk.join()
        out_chk.join()

        f_out_err.close()


# A node.
#
class node_connection( threading.Thread):

    def __init__( self, max_jobs):
        threading.Thread.__init__( self)
        self.max_jobs = max_jobs
        self.queue = Queue()
        self.jobs = []
        self.jobs_lock = threading.RLock()

        new_node = node_alloc()
        self.alloc_id = new_node[0]
        self.node_id = new_node[1]
        
        self.life = threading.Event()
        self.life.set()
        
    def run( self):

        while self.life.is_set() or not( self.jobs):
            new_job = self.queue.get()

            if ( isinstance( new_job, str)):
                if ( info == 'kill'):
                    self.life.clear()

            else:

                # new_job = [jobID, job_type, job_info]
                j = job_send( self.node_id, new_job[2], new_job[0])
                j.start()
                self.jobs_lock.acquire()
                self.jobs.append(j)
                self.jobs_lock.release()

        sleep(1)
        m.exit()
        node.close()


    def kill( self):
        print 'queue_master_driver: killing node connection'
        self.queue.put( 'kill')


# Control jobs subimission
# 
#
class submission_control( threading.Thread):
    def __init__( self):
        threading.Thread.__init__( self)
        self.node_list = []
        self.max_nodes = 3
        self.max_jobs_default = 20
        self.jobs_queue = Queue()
        self.queue_lock = threading.RLock()

    def run( self):
        while True:
            
            # Check finished jobs
            for node_c in self.node_list:
                node_c.jobs_lock.acquire()
                for running_job in node_c.jobs:
                    if (not (running_job.is_alive())):
                        node_c.jobs.remove(running_job)
                node_c.jobs_lock.release()

            # Dealloc the non-used node
            for node_c in self.node_list:
                if (not( node_c.jobs)):
                    node_dealloc( node_c.node_id, node_c.alloc_id)
                    self.node_list.remove( node_c)

            # Send a job, if there is space
            self.queue_lock.acquire()
            if (not( self.jobs_queue.empty())):
                best_free = 0
                best_node = None
                for node_c in self.node_list:
                    free = node_c.max_jobs - len(node_c.jobs)
                    if (free > best_free):
                        best_node = node_c
                        best_free = free
                if (best_node == None and len( self.node_list) < self.max_nodes):
                    best_node = node_connection( self.max_jobs_default)
                    best_node.start()
                    sub_ctrl.node_list.append( best_node)
                if (best_node != None):
                    best_node.queue.put( self.jobs_queue.get())

            self.queue_lock.release()


            sleep(1)

# Get job from client and send it for submission
# message from client must be:
#   (job_type, job)
# where:
#   job_type is
#       1 - submit a job     (sub)
#       2 - check the jobs   (check)
#       3 - kill a job       (kill)
#       4 - kill the master  (finish)
#       5 - change max_nodes (nodes)
#       6 - change max_jobs  (njobs)
#              
#   job is a list with the job informations
#
def handle_client( sub_ctrl, jobId):
    print "queue_master_driver: ready"
    server_master = Listener(( "localhost", port), authkey = 'qwerty')
    while True:
        client_master = server_master.accept()
        (job_type, job) = client_master.recv()

        # Send a job
        if (job_type == 1):
            sub_ctrl.queue_lock.acquire()
            sub_ctrl.jobs_queue.put( (jobId, job_type, job))
            sub_ctrl.queue_lock.release()
            jobId += 1
            client_master.send( 'Job received.\n')

        # Check jobs
        elif (job_type == 2):

            nd_jobs = ''
            cur_jobs = ''

            sub_ctrl.queue_lock.acquire()
            n_q = sub_ctrl.jobs_queue.qsize()
            for i in range( 0, n_q):
                j = sub_ctrl.jobs_queue.get()
                nd_jobs += str( j[0]) + ':' + j[2][0] + ' (wd: ' + j[2][1] + ')\n';
                sub_ctrl.jobs_queue.put( j)
            sub_ctrl.queue_lock.release()

            if (nd_jobs):
                cur_jobs += 'Queue' + ':\n' + nd_jobs

            for node in sub_ctrl.node_list:
                nd_jobs = ''
                for j in node.jobs:
                    nd_jobs += str( j.jobID) + ':' + j.job[0] + ' (wd: ' + j.job[1] + ')\n';
                if (nd_jobs):
                    cur_jobs += node.node_id + ':\n' + nd_jobs

            client_master.send( cur_jobs)

        # Kill a job
        elif (job_type == 3):
#            for i in job:
#                kill i
            client_master.send( 'Killing job: not implemented.\n')


        # Kill the master execution
        if (job_type == 4):
            client_master.send( 'Stopping master driver.\n')

        # Change maximum number of nodes
        elif (job_type == 5):
            sub_ctrl.max_nodes = job
            client_master.send( 'Maximum number of nodes changed to ' + str( job) + '.\n')

        # Change maximum number of jobs
        elif (job_type == 6):
            sub_ctrl.max_jobs_default = job
            for n in sub_ctrl.node_list:
                n.max_jobs = sub_ctrl.max_jobs_default
            client_master.send( 'Maximum number of jobs changed to ' + str( job) + '.\n')

        else:
            client_master.send( 'Unknown option: ' + str( job_type) + '\n')


sub_ctrl = submission_control()
sub_ctrl.start()

#node_dealloc('26402')


handle_client( sub_ctrl, 1)
print "queue_master_driver: done!"
